{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sentiment Analyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will build a seniment analyzer to predict positive or negative sentiment in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siddarth\\Anaconda_main\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\Siddarth\\Anaconda_main\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import numpy as np \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "#rstrip() returns a copy of the string with the trailing characters stripped \n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read())\n",
    "positive_reviews = positive_reviews.find_all('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.find_all('review_text')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We want to have the same size of positive reveiws as negative reviews \n",
    "\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s): \n",
    "    s = s.lower() \n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "word_index_map = {} \n",
    "current_index = 0\n",
    "\n",
    "positive_tokenized = [] \n",
    "negative_tokenized = [] \n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.81\n",
      "you 1.02809303206\n",
      "'ve 0.718761740328\n",
      "sound 0.871813652647\n",
      "customer -0.686587948775\n",
      "n't -1.95140140923\n",
      "feature 0.56047607114\n",
      "time -0.575352437301\n",
      "cable 0.634588844091\n",
      "wa -1.63658347207\n",
      "unit -0.794908729982\n",
      "easy 1.66419967644\n",
      "week -0.700382204525\n",
      "little 0.837124115315\n",
      "bit 0.651967177135\n",
      "doe -1.13104863719\n",
      "lot 0.728633601866\n",
      "using 0.525306674384\n",
      "price 2.61271518905\n",
      "mouse 0.555875550044\n",
      "tried -0.787866228372\n",
      "then -1.08801904046\n",
      "ha 0.782388166282\n",
      "perfect 1.0099092082\n",
      "bad -0.759067810063\n",
      "company -0.519444107502\n",
      "month -0.84459246804\n",
      "pretty 0.735965324222\n",
      "love 1.20682570611\n",
      "quality 1.44194276385\n",
      "picture 0.654460538867\n",
      "try -0.660118928779\n",
      "excellent 1.31558517618\n",
      "money -1.04730320398\n",
      "space 0.585682602902\n",
      "video 0.562396754571\n",
      "item -0.963174630696\n",
      "recommend 0.613562695967\n",
      "paper 0.661029488001\n",
      "speaker 0.832983775257\n",
      "junk -0.572591580393\n",
      "hour -0.581210758766\n",
      "card -0.517200196397\n",
      "fast 0.802754839538\n",
      "highly 0.879702309911\n",
      "buy -0.834864038798\n",
      "comfortable 0.565222741993\n",
      "support -0.926546526799\n",
      "happy 0.639778673541\n",
      "memory 0.915670386079\n",
      "returned -0.793167760237\n",
      "return -1.17774583526\n",
      "value 0.571617011646\n",
      "returning -0.515077982924\n",
      "static -0.508952054534\n",
      "radio -0.52402064071\n",
      "poor -0.77968624658\n",
      "useless -0.500680991732\n",
      "warranty -0.625126659773\n",
      "stopped -0.555746178982\n",
      "waste -1.00374741465\n",
      "refund -0.65691140979\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_vector(tokens, label): \n",
    "    x = np.zeros(len(word_index_map) + 1)\n",
    "    for t in tokens: \n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1 \n",
    "    x = x/x.sum()\n",
    "    x[-1] = label\n",
    "    return (x) \n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0 \n",
    "for tokens in positive_tokenized: \n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy \n",
    "    i += 1 \n",
    "    \n",
    "for tokens in negative_tokenized: \n",
    "    xy = tokens_to_vector(tokens,0)\n",
    "    data[i,:] = xy \n",
    "    i += 1 \n",
    "    \n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100, ]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain )\n",
    "\n",
    "print (\"Classification rate:\", model.score(Xtest, Ytest))\n",
    "\n",
    "threshold = 0.5 \n",
    "for word, index in word_index_map.items(): \n",
    "    weight =model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold: \n",
    "        print (word, weight)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
